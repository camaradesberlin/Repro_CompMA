# Introduction / Aim

<!--Introduce the study and describe which part(s) you are trying to replicate / reproduce and any relevant background information.-->

Being able to reliably reproduce analyses is an important aspect of a robust evidence creation pipeline. Computational reproducibility does not only depend on FAIR data and openly shared code but also on precise descriptions on how the analyses were conducted along with a clear description of the data. [@wilkinson_2016] In fact, recent publications found that only 11 - 24% of code being published along with biomedical manuscripts could be executed withour error. [@ioannidis_2009; @trisovic_2022]
In 2021, Ripley et al. published a preclinical systematic review and meta-analysis on the neuroprotective effects of remote ischemic conditioning (RIC) in rodent models of stroke [@ripley_2021]. Part of a sequence of several reviews on RIC [@hansen_2021; @mollet_2022; @weir_2021], it was chosen by the authors of the current report as the most robust review to be compared with the effect estimate of RIC from the Stroke Preclinical Assessment Network 1.0 [@lyden_2022; @lyden_2023], which tested six neuroprotective strategies in a robust multiarm multistage design. Even though the study adhered to good systematic review methodology, we aimed to investigate whether we could replicate their results based on their shared raw data. Therefore, our goal was to computationally reproduce their meta-analysis process and replicate their primary endpoint as well as all subgroup analyses.
