# Discussion / Conclusion

<!-- What was your experience attempting to replicate / reproduce the study?
What are your recommendations for the reproducibility of future systematic reviews and/or meta-analyses of in vitro or in vivo studies? -->

All analyses were provided by the authors in great detail while only parts of it was freely accessible with the publication. The authors used Comprehensive Meta-Analysis (CMA (RRID:SCR_012779)), a software package requiring a commercial license. Descriptions for statistical analyses were given in the manuscript but lacked details with respect to effect estimators used or any adjustments done in the analyses. We calculated the results using the default settings from the metagen() function from the 'meta' package. Using our approach, our results suggest the same conclusion but the actual effect estimate is outside of the 10% difference that we defined as a successful replication. We would therefore judge our exact replication attempt unsuccessful without any effect on the overall implications of the systematic review from Ripley et al.

There were some imprecisions in the description of analytical decisions made that placed constraints in our ability to reproduce the results from the source manuscript. The description of which experimental comparisons to focus on when those outnumbered the number of animals in the control group, were not as transparently reported in the publication and thus more challenging to reproduce. Nevertheless, sufficient methodological description in the supplementary material with used cohorts being highlighted enabled us to reproduce the original methods. Last, differences in the software used most likely also contributed to the found differences. 

Based on our experiences with this reproducibility task, we recommend to a) be specific on any expert opinion-based decisions and provide reasoning on how such decisions were made. In the above example, cohorts with the highest relevance were chosen but no details were provided on how this decision was made or how relevance was defined. Further, we recommend to b) provide more details on any effect estimator used such as 'Maximum likelihood' or 'Restricted maximum likelihood' as well as any adjustment in the model such as 'Knapp & Hartung' or any others. Including precise description of analytical decisions does not only help to reproduce findings but also gives insights about any assumptions made by the authors on the data in general. 

For computational reproducibility attempts, it would be helpful to derive an overall agreement on how to define a successful replication. We here defined a success somehow arbitrary although we do believe that a 10% difference is meaningful and does provide a basis to assess the reasons for this difference in more detail. Future attempts will most likely in many cases not be able to exactly mirror the same quantitative result as random noise as part of different software will occur. Therefore, a cut-off that distincts random noise from a failed replication will help to design appropriate guidance for How-to guides on computational reproducibility.